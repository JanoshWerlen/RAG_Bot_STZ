{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Introduction to ML and AI with a RAG-System\n",
    "\n",
    "Based on a PDF containing a starter set of DND 5e character [sheets](https://dnd5echaractersheet.com/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sys admin\n",
    "\n",
    "Create a .env file with the following content:\n",
    "\n",
    "`OPENAI_API_KEY = \"^<API_KEY>\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PDF data\n",
    "Loads the data and splits it into chunks.\n",
    "Each chunk contains 1000 characters max with a max overlap of 100 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "loader = PyPDFLoader(\"data/KAR.pdf\")\n",
    "chunks = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the chunks\n",
    "get chunk content with: chunks[index].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='177.4001Reglement über die Anstellungsverhältnisse \\nvon Kaderärztinnen und -ärzten in den Stadt -\\nspitälern (Kaderärztinnen- und -ärzteregle -\\nment, KAR)\\nStadtratsbeschluss vom 26. Januar 2005 (91)  \\nmit Änderung vom 16.  November 2016 (922)\\nGestützt auf Art. 58, Art. 81 Abs. 1 und Art. 87 Abs. 1 des Perso -\\nnalrechts (PR) vom 28. November 2001 wird folgendes Regle -\\nment erlassen:\\nArt. 1 Geltungsbereich\\n1 Dieses Reglement gilt für die in den Stadtspitälern Waid und \\nTriemli angestellten Kaderärztinnen und -ärzte, mit Ausnahme \\nder Chefärztinnen und -ärzte.\\n2 Als Kaderärztinnen und -ärzte im Sinne dieser Bestimmungen \\ngelten Leitende Ärztinnen und Ärzte.1\\n3 Die Vorsteherin bzw. der Vorsteher des Gesundheits- und Um -\\nweltdepartements kann das Reglement auch für Kaderärztinnen \\nund -ärzte in anderen städtischen Betrieben mit Spitalcharakter \\nanwendbar erklären.\\n4 Das Reglement gilt nicht für Kaderärztinnen und -ärzte in Ver -\\nwaltungs- oder anderen nichtärztlichen Funktionen.' metadata={'source': 'data/KAR.pdf', 'page': 0}\n",
      "The chunk contains 988 characters\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0])\n",
    "print(\"The chunk contains \" + str(len(chunks[2].page_content)) + \" characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup models\n",
    "\n",
    "We need to prepare an embedding model to vectorise our chunks before storing them into our ChromaDB and a language model to generate answers to our questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the OpenAI chat model\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8)\n",
    "\n",
    "# initialize the OpenAI embeddings model\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load / Create Chroma DB\n",
    "\n",
    "We check for the existence of the directory for 2 reasons:\n",
    "1) We use Openai Embeddings and pay for the embedding generation\n",
    "2) Chroma does not overwrite an existing database, but allows to upate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Chroma from disk...\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"chroma\"):\n",
    "    print(\"Loading Chroma from disk...\")\n",
    "    chroma_db = Chroma(persist_directory=\"chroma\", embedding_function=embeddings)\n",
    "else:\n",
    "    chroma_db = Chroma.from_documents(documents=chunks,\n",
    "                                    embedding=embeddings,\n",
    "                                    persist_directory=\"chroma\",\n",
    "                                    collection_name=\"lc_chroma_demo\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db = Chroma.from_documents(documents=chunks,\n",
    "                                    embedding=embeddings,\n",
    "                                    persist_directory=\"chroma\",\n",
    "                                    collection_name=\"lc_chroma_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Chroma\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"chroma_db\"):\n",
    "    print(\"Loading Chroma\")\n",
    "    chroma_db =Chroma(persist_directory=\"chroma_db\", embedding_function= embeddings )\n",
    "else: \n",
    "    print(\"Creat new Chroma\")\n",
    "    chroma_db = Chroma.from_documents(chunks, embeddings, persist_directory=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is this document about?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='54 Bei der Ermittlung der Ist-Arbeitszeit sind Abwesenheiten mit \\nLohnanspruch (infolge von Krankheit, Unfall, Militär- oder Zivil -\\nschutzdienst, bezahltem Urlaub usw.) nicht als Arbeitszeit an -\\nzurechnen, wenn sie auf geplante Ruhetage oder, bei Fehlen \\neiner Planung, wenn sie auf Samstage, Sonntage sowie auf Fei -\\ner- oder Betriebsferientage fallen.\\nArt. 8 Inkonvenienzentschädigungen\\nDie Vorsteherin bzw. der Vorsteher des Gesundheits- und Um -\\nweltdepartements wird ermächtigt, Inkonvenienzentschädigungen \\nunter Berücksichtung der kantonalen Regelungen festzusetzen.\\nArt. 9 Schlussbestimmungen\\n1 Dieses Reglement tritt auf den 1. Februar 2005 in Kraft.\\n2 Mit dem Inkrafttreten dieses Reglements wird das Reglement \\nüber die Anstellungsverhältnisse von Kaderärztinnen und -ärz -\\nten in den Stadtspitälern vom 18.  Dezember 2002 aufgehoben.\\nArt. 10 (aufgehoben)4 \\n4 Fassung gem. STRB Nr. 922/2016 vom 16. November 2016; Inkraftsetzung \\n1. Januar 2017.', metadata={'page': 4, 'source': 'data/KAR.pdf'}), Document(page_content='177.4001Reglement über die Anstellungsverhältnisse \\nvon Kaderärztinnen und -ärzten in den Stadt -\\nspitälern (Kaderärztinnen- und -ärzteregle -\\nment, KAR)\\nStadtratsbeschluss vom 26. Januar 2005 (91)  \\nmit Änderung vom 16.  November 2016 (922)\\nGestützt auf Art. 58, Art. 81 Abs. 1 und Art. 87 Abs. 1 des Perso -\\nnalrechts (PR) vom 28. November 2001 wird folgendes Regle -\\nment erlassen:\\nArt. 1 Geltungsbereich\\n1 Dieses Reglement gilt für die in den Stadtspitälern Waid und \\nTriemli angestellten Kaderärztinnen und -ärzte, mit Ausnahme \\nder Chefärztinnen und -ärzte.\\n2 Als Kaderärztinnen und -ärzte im Sinne dieser Bestimmungen \\ngelten Leitende Ärztinnen und Ärzte.1\\n3 Die Vorsteherin bzw. der Vorsteher des Gesundheits- und Um -\\nweltdepartements kann das Reglement auch für Kaderärztinnen \\nund -ärzte in anderen städtischen Betrieben mit Spitalcharakter \\nanwendbar erklären.\\n4 Das Reglement gilt nicht für Kaderärztinnen und -ärzte in Ver -\\nwaltungs- oder anderen nichtärztlichen Funktionen.', metadata={'page': 0, 'source': 'data/KAR.pdf'}), Document(page_content='2Art. 3 Honorare der Kaderärztinnen und -ärzte\\n1 Die Honorarberechtigung und -bewilligung wird separat geregelt.\\n2 Kaderärztinnen und -ärzte können auf die Honorarbewilligung \\nverzichten. Wird der Verzicht nicht bereits bei der Anstellung \\noder einem Funktionsstufenwechsel erklärt, so kann die ent -\\nsprechende Erklärung unter Wahrung einer dreimonatigen Frist \\nauf den Beginn eines Quartals schriftlich erfolgen. Die gleichen \\nFristen gelten für einen Widerruf des Verzichts.\\n3 Der Verzicht auf die Honorarbewilligung bewirkt nicht den \\nVerlust des Rechts, in Absprache mit der Chefärztin oder dem \\nChefarzt Privat- oder Halbprivatpatientinnen und -patienten zu \\nbehandeln, und führt auch nicht zu Änderungen in der Rech -\\nnungsstellung, sondern hat lediglich zur Folge, dass die entspre -\\nchenden Honorarguthaben voll dem Spital zufallen.\\nArt. 4 Arbeitszeit\\n1 Die Höchstarbeitszeit der Kaderärztinnen und -ärzte mit Hono -\\nrarbewilligung beträgt im Quartalsdurchschnitt 65  Stunden pro', metadata={'page': 1, 'source': 'data/KAR.pdf'}), Document(page_content='rarbewilligung beträgt im Quartalsdurchschnitt 65  Stunden pro \\nWoche, jene der Kaderärztinnen und -ärzte ohne Honorarbe -\\nwilligung 55 Stunden. Die Chefärztinnen oder Chefärzte können \\nfür ihre Klinik oder ihr Institut der Spitalleitung Antrag auf Her -\\nabsetzung der Höchstarbeitszeiten auf bis 55 bzw. 50 Stunden \\nstellen, wenn es die betrieblichen Verhältnisse erlauben.\\n2 Für die Fortbildung haben die Kaderärztinnen und -ärzte An -\\nspruch auf höchstens zehn Arbeitstage pro Facharzttitel und \\nJahr.\\n3 Die ununterbrochene Anwesenheit im Betrieb darf grundsätz -\\nlich 24  Stunden nicht überschreiten; Notfälle bleiben vorbehal -\\nten. Soweit dies für eine ordentliche Übergabe unerlässlich ist, \\nkönnen diese Zeiten um höchstens eine Stunde überschritten \\nwerden. Es ist anschliessend eine angemessene Ruhezeit zu \\ngewähren.\\n4 Die Vorsteherin bzw. der Vorsteher des Gesundheits- und Um -\\nweltdepartements wird ermächtigt, die Höchstarbeitszeit zu re -', metadata={'page': 1, 'source': 'data/KAR.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "result = chroma_db.similarity_search(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity Search with Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_with_scores = chroma_db.similarity_search_with_score(query)\n",
    "print(result_with_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=chroma_db.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_file_path = \"responses.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke(query)\n",
    "print(response[\"result\"])\n",
    "\n",
    "with open(response_file_path, \"a\", encoding=\"utf-8\") as response_file:\n",
    "    response_file.write(\"1 \\n\"+response[\"query\"] +\"\\n\" +response[\"result\"] + \"\\n\")\n",
    "\n",
    "print(\"Response appended to\", response_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test some queries Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(query:str):\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=chroma_db.as_retriever())\n",
    "    response = chain.invoke(query)\n",
    "    print(response[\"result\"])\n",
    "    print(response)\n",
    "\n",
    "    with open(response_file_path, \"a\", encoding=\"utf-8\") as response_file:\n",
    "        response_file.write(\"// \\n\"+response[\"query\"] +\"\\n\" +response[\"result\"] +\"//\"+ \"\\n\")\n",
    "\n",
    "    print(\"Response appended to\", response_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_response(\"Was ist der Inhalt des Dokumentes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Chroma\n",
      "This document is a regulation regarding the employment conditions of senior physicians (Kaderärztinnen and -ärzte) in city hospitals, specifically the Waid and Triemli hospitals. It covers aspects such as working hours, honorarium regulations, and the scope of application of the regulation.\n",
      "Response appended to responses.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This document is a regulation regarding the employment conditions of senior physicians (Kaderärztinnen and -ärzte) in city hospitals, specifically the Waid and Triemli hospitals. It covers aspects such as working hours, honorarium regulations, and the scope of application of the regulation.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "chroma_db = None\n",
    "llm = None\n",
    "\n",
    "def initialise_AI():\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    loader = PyPDFLoader(\"data/Personalrecht.pdf\")\n",
    "    chunks = loader.load_and_split(text_splitter)\n",
    "\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Access the API key using the variable name defined in the .env file\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Initialize the OpenAI chat model\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    if os.path.exists(\"chroma_db\"):\n",
    "        print(\"Loading Chroma\")\n",
    "        chroma_db =Chroma(persist_directory=\"chroma_db\", embedding_function= embeddings )\n",
    "    else: \n",
    "        print(\"Creat new Chroma\")\n",
    "        chroma_db = Chroma.from_documents(chunks, embeddings, persist_directory=\"chroma_db\")\n",
    "        \n",
    "    return chroma_db, llm\n",
    "        \n",
    "\n",
    "def get_response(query:str):\n",
    "\n",
    "    response_file_path = \"responses.txt\"\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=chroma_db.as_retriever())\n",
    "    response = chain.invoke(query)\n",
    "    print(response[\"result\"])\n",
    "\n",
    "    with open(response_file_path, \"a\", encoding=\"utf-8\") as response_file:\n",
    "        response_file.write(\"// \\n\" + response[\"query\"] + \"\\n\" + response[\"result\"] + \"//\" + \"\\n\")\n",
    "\n",
    "    print(\"Response appended to\", response_file_path)\n",
    "\n",
    "    return response[\"result\"]\n",
    "  \n",
    "        \n",
    "\n",
    "# Call initialise_AI before get_response\n",
    "chroma_db, llm = initialise_AI()\n",
    "get_response(\"What is this document about?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from bots import get_response\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "\n",
    "def do_chat(query:str):\n",
    "\n",
    "    context = get_response(query, \"Alle\")\n",
    "\n",
    "    chat = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": f\"You are a HR assistant and base your answers on this context: {context}\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    response = chat.choices[0].message.content\n",
    "    print(response)\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from datetime import datetime\n",
    "\n",
    "chroma_db = None\n",
    "chroma_db_PR = None\n",
    "chroma_db_KAR = None\n",
    "llm = None\n",
    "\n",
    "\n",
    "response_file_path = \"responses.txt\"\n",
    "pdf_folderpath = \"data\"\n",
    "\n",
    "\n",
    "def initialise_AI():\n",
    "\n",
    "    global pdf_folderpath\n",
    "    global chroma_db\n",
    "    global chroma_db_PR\n",
    "    global chroma_db_KAR\n",
    "    global llm\n",
    "\n",
    "    print(\"initialise_text_splitter\")\n",
    "\n",
    "    documents_PR = []\n",
    "    documents_KAR = []\n",
    "\n",
    "    for file in os.listdir(pdf_folderpath):\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folderpath, file)\n",
    "            loader = PyPDFLoader(pdf_path)\n",
    "            if \"PR\" in file:\n",
    "                documents_PR.extend(loader.load())\n",
    "            elif \"KAR\" in file:\n",
    "                documents_KAR.extend(loader.load())\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "    print(f\"PR documents: {len(documents_PR)}\")\n",
    "    print(f\"KAR documents: {len(documents_KAR)}\")\n",
    "\n",
    "    # Split documents\n",
    "    chunks_PR = text_splitter.split_documents(documents_PR)\n",
    "    chunks_KAR = text_splitter.split_documents(documents_KAR)\n",
    "\n",
    "    # Combine all documents for the general database\n",
    "    all_documents = documents_PR + documents_KAR\n",
    "    chunks_all = text_splitter.split_documents(all_documents)\n",
    "\n",
    "    print(f\"Alle {len(chunks_all)}\")\n",
    "    print(f\"KAR {len(chunks_KAR)}\")\n",
    "    print(f\"PR {len(chunks_PR)}\")\n",
    "\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Access the API key using the variable name defined in the .env file\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Initialize the OpenAI chat model\n",
    "    print(\"initialise_llm\")\n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.8)\n",
    "\n",
    "    # Check for or create databases\n",
    "    chroma_db = check_for_db(chunks_all, \"chroma_db\")\n",
    "    chroma_db_PR = check_for_db(chunks_PR, \"chroma_db_PR\")\n",
    "    chroma_db_KAR = check_for_db(chunks_KAR, \"chroma_db_KAR\")\n",
    "\n",
    "\n",
    "def create_timestamp():\n",
    "    current_timestamp = datetime.timestamp(datetime.now())\n",
    "    dt_object = datetime.fromtimestamp(current_timestamp)\n",
    "    formatted_date_time = dt_object.strftime(\"%d/%m/%Y %H:%M\")\n",
    "\n",
    "    return formatted_date_time\n",
    "\n",
    "\n",
    "def create_log(timestamp, response):\n",
    "\n",
    "    with open(response_file_path, \"a\", encoding=\"utf-8\") as response_file:\n",
    "        response_file.write(f\"//{timestamp} \\n\" + \"Frage: \" +\n",
    "                            response[\"query\"] + \"\\n\" + response[\"result\"] + \"//\" + \"\\n\")\n",
    "\n",
    "    print(\"Response appended to\", response_file_path)\n",
    "\n",
    "\n",
    "def get_response(query: str, categorie):\n",
    "\n",
    "    global chroma_db\n",
    "    global chroma_db_PR\n",
    "    global chroma_db_KAR\n",
    "    global llm\n",
    "\n",
    "    if categorie == \"Alle\":\n",
    "        print(\"Gewählte Ketegorie ALLE\")\n",
    "        chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm, chain_type=\"stuff\", retriever=chroma_db.as_retriever())\n",
    "        response = chain.invoke(query)\n",
    "        print(response[\"result\"])\n",
    "\n",
    "        formatted_date_time = create_timestamp()\n",
    "\n",
    "        create_log(formatted_date_time, response)\n",
    "\n",
    "        return response[\"result\"]\n",
    "    \n",
    "    elif categorie == \"PR\":\n",
    "        print(\"Gewählte Ketegorie PR\")\n",
    "        chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm, chain_type=\"stuff\", retriever=chroma_db_PR.as_retriever())\n",
    "        response = chain.invoke(query)\n",
    "        print(response[\"result\"])\n",
    "\n",
    "        formatted_date_time = create_timestamp()\n",
    "\n",
    "        create_log(formatted_date_time, response)\n",
    "\n",
    "        return response[\"result\"]\n",
    "    \n",
    "    elif categorie == \"KAR\":\n",
    "        print(\"Gewählte Ketegorie KAR\")\n",
    "        chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm, chain_type=\"stuff\", retriever=chroma_db_KAR.as_retriever())\n",
    "        response = chain.invoke(query)\n",
    "        print(response[\"result\"])\n",
    "\n",
    "        formatted_date_time = create_timestamp()\n",
    "\n",
    "        create_log(formatted_date_time, response)\n",
    "\n",
    "        return response[\"result\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_chroma_db(embeddings, name: str):\n",
    "    print(f\"Loading Chroma ({name})\")\n",
    "    chroma_db = Chroma(persist_directory=name, embedding_function=embeddings)\n",
    "    return chroma_db\n",
    "\n",
    "def create_chroma_db(chunks, embeddings, name: str):\n",
    "    print(f\"Creating new Chroma ({name}) with {len(chunks)}\")\n",
    "    \n",
    "    chroma_db = Chroma.from_documents(chunks, embeddings, persist_directory=name)\n",
    "    return chroma_db\n",
    "\n",
    "def check_for_db(chunks, name: str):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    global chroma_db\n",
    "    global chroma_db_PR\n",
    "    global chroma_db_KAR\n",
    "\n",
    "    if os.path.exists(name):\n",
    "        print(f\"{name} exists\")\n",
    "        if name == \"chroma_db\":\n",
    "            chroma_db = get_chroma_db(embeddings, name)\n",
    "        elif name == \"chroma_db_PR\":\n",
    "            chroma_db_PR = get_chroma_db(embeddings, name)\n",
    "        elif name == \"chroma_db_KAR\":\n",
    "            chroma_db_KAR = get_chroma_db(embeddings, name)\n",
    "    else:\n",
    "        if name == \"chroma_db\":\n",
    "            chroma_db = create_chroma_db(chunks, embeddings, name)\n",
    "        elif name == \"chroma_db_PR\":\n",
    "            chroma_db_PR = create_chroma_db(chunks, embeddings, name)\n",
    "        elif name == \"chroma_db_KAR\":\n",
    "            chroma_db_KAR = create_chroma_db(chunks, embeddings, name)\n",
    "        print(f\"Created {name}\")\n",
    "\n",
    "    if name == \"chroma_db\":\n",
    "        return chroma_db\n",
    "    elif name == \"chroma_db_PR\":\n",
    "        return chroma_db_PR\n",
    "    elif name == \"chroma_db_KAR\":\n",
    "        return chroma_db_KAR\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialise_text_splitter\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initialise_AI\n\u001b[1;32m----> 3\u001b[0m \u001b[43minitialise_AI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\VS_Code\\ZHAW\\ML2\\RAG\\RAG-week1\\bots_code\\bots.py:33\u001b[0m, in \u001b[0;36minitialise_AI\u001b[1;34m()\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data'"
     ]
    }
   ],
   "source": [
    "from bots import initialise_AI\n",
    "\n",
    "initialise_AI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gewählte Ketegorie ALLE\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'as_retriever'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdo_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho are you?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m, in \u001b[0;36mdo_chat\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_chat\u001b[39m(query:\u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAlle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     chat \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     12\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     13\u001b[0m         messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m         ]\n\u001b[0;32m     19\u001b[0m     )\n\u001b[0;32m     20\u001b[0m     response \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32me:\\VS_Code\\ZHAW\\ML2\\RAG\\RAG-week1\\bots_code\\bots.py:103\u001b[0m, in \u001b[0;36mget_response\u001b[1;34m(query, categorie)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m categorie \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAlle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGewählte Ketegorie ALLE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    102\u001b[0m     chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m--> 103\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m, retriever\u001b[38;5;241m=\u001b[39m\u001b[43mchroma_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m())\n\u001b[0;32m    104\u001b[0m     response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'as_retriever'"
     ]
    }
   ],
   "source": [
    "do_chat(\"Who are you?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
